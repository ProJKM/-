# -*- coding: utf-8 -*-
"""오픈소스 분해.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egI6ZJu7wmJV55eMYW2RQHFn0yzKuHML
"""

!pip install tensorflow==1.14.0

!pip install keras==2.3.1

import sys
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
import numpy as np
from matplotlib import pyplot as plt
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping

img_rows = 28
img_cols = 28

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

input_shape = (img_rows, img_cols, 1)
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# x_train= x_train[0:30000]
# x_test= x_test[0:5000]
# y_train= y_train[0:30000]
# y_test= y_test[0:5000]

print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'x_train samples')
print(x_test.shape[0], 'x_test samples')
print(y_train.shape, 'y_train samples')
print(y_test.shape, 'y_test samples')


num_classes=10

y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)




optimizers = [
    'Adadelta',
    'Adam',
    'RMSprop',
    'SGD'
]

op=[]
for optimizer in optimizers:
  model = Sequential()
  model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=input_shape))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.25))
  model.add(Flatten())
  model.add(Dense(50, activation='relu'))
  model.add(Dropout(0.25))
  model.add(Dense(num_classes, activation='softmax'))
  
  early_stopping = EarlyStopping(monitor='loss',patience=3)
  model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])
  history = model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(x_test, y_test),callbacks=[early_stopping])
  op.append(history)

plt.plot(op[0].history['loss'])
plt.plot(op[0].history['val_loss'])
plt.title('{}'.format(optimizers[0]))
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.plot(op[1].history['loss'])
plt.plot(op[1].history['val_loss'])
plt.title('{}'.format(optimizers[1]))
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.plot(op[2].history['loss'])
plt.plot(op[2].history['val_loss'])
plt.title('{}'.format(optimizers[2]))
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.figure(figsize=(4,4))

plt.plot(op[3].history['loss'])
plt.plot(op[3].history['val_loss'])
plt.title('{}'.format(optimizers[3]))
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.figure(figsize=(4,4))